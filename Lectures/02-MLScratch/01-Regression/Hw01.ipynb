{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  27.583877854049504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "m = X.shape[0]  \n",
    "n = X.shape[1] \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, alpha=0.001, max_iter=10000, \n",
    "            loss_old=10000, tol=1e-5, method=\"batch\",size=10):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_old = loss_old\n",
    "        self.tol = tol\n",
    "        self.method = method\n",
    "        self.size  = size\n",
    "        \n",
    "   \n",
    "        \n",
    "    def h_theta(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        iter_stop = 0\n",
    "        mem_i = [] \n",
    "        for i in range(self.max_iter):\n",
    "            if self.method == \"batch\":\n",
    "                X_train = X\n",
    "                y_train = y         \n",
    "            \n",
    "            elif self.method == 'sto':\n",
    "                i = np.random.randint(X.shape[0])\n",
    "                if i in mem_i:\n",
    "                    i = np.random.randint(X.shape[0])\n",
    "                X_train = X[i,:].reshape(1,-1)\n",
    "                y_train = y[i]\n",
    "                mem_i.append(i)\n",
    "                if len(mem_i) == m:\n",
    "                    mem_i = []\n",
    "            \n",
    "            elif self.method == 'mini':\n",
    "                i = np.random.randint(X.shape[0])\n",
    "                if i in mem_i:\n",
    "                    i = np.random.randint(X.shape[0])\n",
    "                X_train = X[i:i+self.size,:]\n",
    "                y_train = y[i:i+self.size]\n",
    "                mem_i.append(i)\n",
    "                if len(mem_i) == m:\n",
    "                    mem_i = []\n",
    "            else:\n",
    "                print('method : batch or sto or mini')\n",
    "                break\n",
    "            \n",
    "            yhat = self.h_theta(X_train)\n",
    "            loss_curr = ((yhat - y_train)**2).sum() / yhat.shape[0]\n",
    "            diff = np.abs(loss_curr-self.loss_old)\n",
    "            if  (diff<self.tol):\n",
    "                iter_stop = i\n",
    "                break\n",
    "            self.loss_old = loss_curr\n",
    "            error = yhat - y_train\n",
    "            grad = self.gradient(X_train, error)\n",
    "            self.theta = self.theta - self.alpha * grad\n",
    "            \n",
    "\n",
    "model = LinearRegression(method=\"sto\")\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.h_theta(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Load the MNIST dataset (introduced in chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "\n",
    "1. Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the accuracy score of the resulting model on the test set.  Attempt to scale the X since PCA will benefit from that.\n",
    "\n",
    "2. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.\n",
    "\n",
    "3. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Why?  Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "\n",
    "4. Repeat the above steps using LogisticRegression with multi-class=\"multinomial\" and solver=\"lbfgs\" .  Was training much faster with PCA?  Do accuracy lower after PCA? (ignore the convergence warning since you can easily set max_iter to higher to remove this warning; here we care for time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. Then try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations\n",
    "\n",
    "When you compare the visualizations, you do not really have any metric, but one way to tell is whether this visualization separate each class well enough for eyes to see.\n",
    "\n",
    "Since t-SNE is even more time-consuming, try to obtain only 5000 images.  And since t-SNE is more about visualization, it does not require you to train-test split here.\n",
    "\n",
    "For MDS, be warned it will take long long time.  Try to reduce to perhaps 1000 images for your good mental state.\n",
    "\n",
    "Once you compare all singular models.  For last model, try a pipeline of \n",
    "\n",
    "<code>pca_tsne = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
    "    (\"tsne\", TSNE(n_components=2, random_state=42)),\n",
    "])</code>\n",
    "\n",
    "So what do you think the above pipeline does? How is the time compared to tsne only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \n",
    "\n",
    "attempt to compare the followings:\n",
    "\n",
    " 1. SVM prediction with PCA of 0.99 explained variances\n",
    " 2. SVM prediction with PCA of 0.90 explained variances\n",
    "\n",
    "- you probably want to use grid search to find the best C and gamma\n",
    "- since image is non-linear, it may be wise to use rbf/poly kernel\n",
    "- since there are imbalanced set of target images, use class_weight='balanced'\n",
    "- for pca, since we are working with images, with many correlated pixels\n",
    "- it is useful to use whiten=True which will transform our covariance matrix to unit matrix (it is similar to standardizing your PCA)\n",
    "\n",
    "\n",
    "How much time differences between the two?\n",
    "\n",
    "How about the accuracy? (Hint: accuracy should be avoided in imbalanced dataset)\n",
    "\n",
    "Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\n",
    "\n",
    "Modify the scratch code of PCA in our lecture:\n",
    "\n",
    "- Modify so instead of using np.linalg.eig; let's modify so it uses SVD approach.\n",
    "- Convert it using kernel PCA, where we convert our X using rbf kernels.  You may want to see how to transfer your data to another space via https://en.wikipedia.org/wiki/Radial_basis_function_kernel and this http://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/#References. For those who are confused what is x and x' (prime), they are basically each sample, so you may want to first find the squared distances between each sample.  Apply your kernel PCA with a sklearn datasets make_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\n",
    "\n",
    "Implement ISOmap from scratch. Reduced to 200 samples for peace of mind.\n",
    "<code>X, color = datasets.make_swiss_roll(n_samples=200, noise=0.3)</code>\n",
    "\n",
    "Since ISOmap is essential very similar to MDS but instead of a simple distance matrix, it feeds in a shortest path matrix instead, hence the geodesic distances.\n",
    "\n",
    "First, compute the distances.  Make sure it is euclidean for this step. Also make sure that the distance matrix should have values for only the nth nearest neighbors. Otherwise, values should be infinity. Infinity will be later on used by Floyd algorithm to determine the shortest paths. Last, treat this distance matrix as undirected graph, which means <code>distance\\[i\\]\\[j\\]</code> should be same as <code>distance\\[j\\]\\[i\\]</code>\n",
    "\n",
    "Second, input the distance matrix into Floyd algorithm. This part can be unfamiliar for those who did not study Data Structures and Algorithms.  \n",
    "Floyd Warshall algorithm is basically a all-pairs shortest-path algorithm. It is pretty straightforward algorithm in which you loop through all possible pair of vertices and their possible paths and keep on updating if shorter path is found. \n",
    "This is implemented based on pseudocode given in https://en.wikipedia.org/wiki/Floydâ€“Warshall_algorithm.\n",
    "For those who want to first get an intuition, watch this https://www.youtube.com/watch?v=4OQeCuLYj-4.\n",
    "Here is how the floyd algorithm can be implemented in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floydWarshall(distance_matrix):\n",
    "    #shorten the name for shorter code!\n",
    "    #I write the long name as param so you guys will know\n",
    "    #what is it that is being input to floyd\n",
    "    dist = distance_matrix\n",
    "    length = len(dist)\n",
    "    for k in range(length):\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                dist[i][j] = min(dist[i][j],dist[i][k] + dist[k][j])\n",
    "    return graph_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, you need to feed the graph matrix to the MDS algorithm. This MDS algorithm is quite straightforward as well.\n",
    "For simplicity, you can follow the steps written in https://en.wikipedia.org/wiki/Multidimensional_scaling in topic \"Steps of a classical MDS algorithm\"\n",
    "\n",
    "1. Squared the incoming matrix return my floyd\n",
    "\n",
    "2. Perform double centering which center the data, and remove asymmetrical distances by multiplying 0.5\n",
    "\n",
    "3. Use elg() to find eigenvalues and eigenvectors\n",
    "\n",
    "4. The new X = Em(Am)^1/2, where EM is eigenvectors and Am is the diagonal matrix of eigenvalues\n",
    "\n",
    "Phew, it may look hard, but it is always satisfying to understand the math behind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement ISOmap from scratch\n",
    "#reduced to 200 samples for peace of mind\n",
    "X, color = datasets.make_swiss_roll(n_samples=200, noise=0.3)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

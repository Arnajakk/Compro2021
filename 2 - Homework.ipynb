{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). <code>baseball = genfromtxt('resources/baseball.csv', delimiter=',',\n",
    "                      encoding=\"utf-8\", dtype=None)\n",
    "print(\"Fields: \", baseball[0])\n",
    "print(\"First name: \", baseball[1][0])\n",
    "print(\"First name: \", baseball[1][0], \" has a weight of\", \n",
    "      baseball[1][4]) </code>\n",
    "\n",
    "- find out all unique team\n",
    "- find out all unique position\n",
    "- average height by team and position (sorted descending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). <code> fifa = genfromtxt('resources/fifa.csv', delimiter=',',\n",
    "                      encoding=\"utf-8\", dtype=None)\n",
    "print(\"Fields: \", fifa[0]) </code>\n",
    "\n",
    "we are scouts who want strikers/forward (A) with left foot, with height\n",
    "more than 190, with shooting over 75.  Basically, he should be able\n",
    "to both shoot and head!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3).  Create a function that compute moving average using a sliding\n",
    "window over a list.  You may want to perform a bit of research what is moving average\n",
    "\n",
    "For example, <code>moving_average([1,2,5,10], n=2)</code> should produce\n",
    "        \n",
    "        [1.5 3.5 7.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4).\n",
    "<code>#Read a digits image and split it up in different training examples\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img = cv2.imread('resources/digits.png', 0)\n",
    "plt.imshow(img)\n",
    "print(img.shape)</code>\n",
    "\n",
    "- first, our img is a numpy array, attempt to break into 100 letters \n",
    "\n",
    "- commonly, we break our dataset to training and testing set.  If our dataset is small, then probably testing should be around 30% then the rest is training set.  When our dataset is really big, 5% for testing set suffice.  Here our dataset is quite small, so attempt to break into 30% testing set, and 70% training set. Result should be X_train, X_test, y_train, y_test.  Code from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)  Create a set of 100 points that follow the function   \n",
    "\n",
    "$$f(x) = 0.5x + 1$$\n",
    "\n",
    "- first, add gaussian white noise to the result using np.random.normal\n",
    "- second, use polyfit to fit the results in a line and plot them using\n",
    "  <code>\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, y, 'b.')  #blue dots\n",
    "plt.plot(X, coeff[1] + coeff[0]*X, 'r')  #overlay the line</code>\n",
    "- third, perform linear regression with ordinary least squares using the closed-form expression matrix formula $$\\mathbf{w} = (\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1} \\mathbf{X}^{\\rm T}\\mathbf{y}$$  See whether we can get back the coefficients 1 and 0.5.  FYI: To invert a matrix, use <code>np.linalg.inv</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6). \n",
    "<code>iris = genfromtxt('resources/iris.csv', delimiter=',',\n",
    "                      encoding=\"utf-8\", dtype=None)\n",
    "iris_without_headers = np.array(iris[1:])\n",
    "species = iris_without_headers[:, 5]\n",
    "sepal_length = iris_without_headers[:, 1].astype(float)\n",
    "petal_length = iris_without_headers[:, 3].astype(float)\n",
    "print(\"Fields: \", iris[0]) </code>\n",
    "\n",
    "- calculate how many samples of data we have\n",
    "- get all unique species\n",
    "- compute the mean and std of sepal length of each species\n",
    "- retrieve data with sepal length less than 5, and petal length greater than 1.5\n",
    "- Check if any missing values in the whole data\n",
    "- find correlation between sepal and petal length\n",
    "- let's perform logistic regression.  For fun, implement from scratch!(Do not use scikit learn!).  Define X as sepal length and Y as 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7). Implement a simple perceptron using Numpy on the given file and output the trained weights, attempt to measure the accuracy of this trained weights.  Use a simple stochastic gradient descent method in which the model make prediction at each training instance where error is calculated.  Then the model is updated in order to reduce the error for the next prediction.  This procedure can be used fo find the set of weights in a model that result in the smallest error.  The formula is as follows:\n",
    "\n",
    "<code>w = w + learning_rate * (expected - predicted) * x #for x\n",
    "w = w + learning_rate * (expected - predicted) #for bias</code>\n",
    "\n",
    "Here is the initial code:\n",
    "\n",
    "<code>data = np.genfromtxt('resources/perceptron.csv', delimiter=',', skip_header=1)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "cond0 = y==0\n",
    "cond1 = y==1\n",
    "plt.plot(X[:,0][cond0], X[:,1][cond0], 'o', mec='r', mfc='none')\n",
    "plt.plot(X[:,0][cond1], X[:,1][cond1], 'o', mec='g', mfc='none')</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
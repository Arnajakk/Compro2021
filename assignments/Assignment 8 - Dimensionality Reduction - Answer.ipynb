{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8 - Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Load the MNIST dataset (introduced in chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "\n",
    "1. Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the accuracy score of the resulting model on the test set.  Attempt to scale the X since PCA will benefit from that.\n",
    "\n",
    "2. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.\n",
    "\n",
    "3. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Why?  Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "\n",
    "4. Repeat the above steps using LogisticRegression with multi-class=\"multinomial\" and solver=\"lbfgs\" .  Was training much faster with PCA?  Do accuracy lower after PCA? (ignore the convergence warning since you can easily set max_iter to higher to remove this warning; here we care for time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8dd141adef56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame)\u001b[0m\n\u001b[1;32m    818\u001b[0m                                     \u001b[0mfeatures_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                                     \u001b[0mtarget_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                                     data_columns=data_columns)\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, features_list, data_columns, target_columns, shape)\u001b[0m\n\u001b[1;32m    559\u001b[0m                              \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                              \u001b[0mencode_nominal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mas_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                              parse_arff=parse_arff)\n\u001b[0m\u001b[1;32m    562\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, return_type, encode_nominal, parse_arff)\u001b[0m\n\u001b[1;32m    464\u001b[0m                           \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                           encode_nominal=encode_nominal)\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mparse_arff\u001b[0;34m(arff)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_arff_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_slice_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_slice_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0;31m# nominal attributes is a dict mapping from the attribute name to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;31m# the possible values. Includes also the target column (which will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_convert_arff_data\u001b[0;34m(arff, col_slice_x, col_slice_y, shape)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         data = np.fromiter(itertools.chain.from_iterable(arff_data),\n\u001b[0;32m--> 253\u001b[0;31m                            dtype='float64', count=count)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_slice_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    471\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mBadDataFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m_decode_values\u001b[0;34m(values, conversors)\u001b[0m\n\u001b[1;32m    478\u001b[0m             values = [None if value is None else conversor(value)\n\u001b[1;32m    479\u001b[0m                       \u001b[0;32mfor\u001b[0m \u001b[0mconversor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                       in zip(conversors, values)]\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'float: '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.7/site-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             values = [None if value is None else conversor(value)\n\u001b[0;32m--> 479\u001b[0;31m                       \u001b[0;32mfor\u001b[0m \u001b[0mconversor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m                       in zip(conversors, values)]\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting...\n",
    "X_train = mnist['data'][:60000]\n",
    "y_train = mnist['target'][:60000]\n",
    "\n",
    "X_test = mnist['data'][60000:]\n",
    "y_test = mnist['target'][60000:]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 1. Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the accuracy_score of resulting model on the test set.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "start = time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Training took: {time()-start:.2f}\")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test) #use the same weights to transform X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Why? Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "\n",
    "start = time()\n",
    "model.fit(X_train_reduced, y_train)\n",
    "print(f\"Training took: {time()-start:.2f}\")\n",
    "\n",
    "pred = model.predict(X_test_reduced)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")\n",
    "\n",
    "\"\"\"\n",
    "Oh no!  It got much slower!  Well, PCA performance depends\n",
    "on the algorithm it used with.  Here it is RandomForest.  It is\n",
    "possible that PCA removes features that are essential\n",
    "to classification in RandomForest. Simply said, PCA extracts\n",
    "what is common in data, but NOT what differentiates them.\n",
    "\n",
    "Another problem is that if you have a good predictor\n",
    "and another variable that is highly correlated to the good\n",
    "predictor, both will be projected to the same dimension, hence\n",
    "noise is added to the good predictor, hence blurring the good variable\n",
    "\n",
    "Lesson learned is that applying PCA must be always done\n",
    "with caution, and best with trial-and-error \n",
    "\n",
    "Bad news is done.  You will find out that PCA actually works\n",
    "well with LogisticRegression, since both kinda share the similar\n",
    "linear form of equation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Repeat the above steps using LogisticRegression with multi-class=\"multinomial\" and solver=\"lbfgs\" .  Was training much faster with PCA?  Do accuracy lower after PCA? (ignore the convergence warning since you can easily set max_iter to higher to remove this warning; here we care for time)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "start = time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Training took: {time()-start:.2f}\")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "model.fit(X_train_reduced, y_train)\n",
    "print(f\"Training took: {time()-start:.2f}\")\n",
    "\n",
    "pred = model.predict(X_test_reduced)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")\n",
    "\n",
    "\"\"\"\n",
    "Wow, almost two-three times faster, with only 0.005 loss in accuracy.\n",
    "I would say it is worth to perform PCA for LogisticRegression,\n",
    "almost always have a try!  How about try SVM and other classification\n",
    "algorithms as well?  I will leave that to you.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. Then try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations\n",
    "\n",
    "When you compare the visualizations, you do not really have any metric, but one way to tell is whether this visualization separate each class well enough for eyes to see.\n",
    "\n",
    "Since t-SNE is even more time-consuming, try to obtain only 5000 images.  And since t-SNE is more about visualization, it does not require you to train-test split here.\n",
    "\n",
    "For MDS, be warned it will take long long time.  Try to reduce to perhaps 1000 images for your good mental state.\n",
    "\n",
    "Once you compare all singular models.  For last model, try a pipeline of \n",
    "\n",
    "<code>pca_tsne = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
    "    (\"tsne\", TSNE(n_components=2, random_state=42)),\n",
    "])</code>\n",
    "\n",
    "So what do you think the above pipeline does? How is the time compared to tsne only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 5000\n",
    "#randomly permuate np.arange(60000), and take only the first m results\n",
    "ix = np.random.permutation(60000)[:m]\n",
    "\n",
    "X = mnist.data[ix]\n",
    "#y comes in String so remember to convert to int\n",
    "y = mnist.target[ix].astype(int)\n",
    "\n",
    "#scaling always help almost all distance algorithms\n",
    "#which include all manifold learning algorithms as well as PCA\n",
    "#will help especially if we have outliers or distant features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "start = time()\n",
    "X_reduced = tsne.fit_transform(X)\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "def plot_clusters(X_reduced, y):\n",
    "    plt.figure(figsize=(13,10))\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"jet\")\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "plot_clusters(X_reduced, y)\n",
    "\n",
    "# It took 55-ish seconds!!! to get this visualization.  However, as we look at it, TSNE did quite a good job separating these numbers using only 2 components (especially, 0, 6, and 8).  On the other hand, 4 and 9; 5 and 3, among others are quite difficult to visualize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2, random_state=42)\n",
    "start = time()\n",
    "X_reduced = model.fit_transform(X)\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "plot_clusters(X_reduced, y)\n",
    "\n",
    "# Training took a lot less time, but it is not so linearly separable. Well, at least 0 has some clear boundary - mostly on the right. Let's try other algoritms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLE\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "model = LocallyLinearEmbedding(n_components=2, random_state=42)\n",
    "start = time()\n",
    "X_reduced = model.fit_transform(X)\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "plot_clusters(X_reduced, y)\n",
    "\n",
    "# Hmm...this result seems to be slighly better than PCA.  We can see that 0 and 6 are quite separable but that's it!....it also take a lot of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MDS\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "model = MDS(n_components=2, random_state=42)\n",
    "start = time()\n",
    "X_reduced = model.fit_transform(X[:1000])\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "plot_clusters(X_reduced, y[:1000])\n",
    "\n",
    "# Never run MDS again....the time is just too horrible to think about the visualization, right?  The reason for its time is due to the fact that it has to calculate the distance between all pairs and this process will take a lot of time naturally.  Well, MDS also is not doing very well in discriminating the features either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "# LDA is originally for classification but it can also be equally applied as a dimensionality reduction technique by specifying the n_components.  The difference between LDA and PCA is that LDA finds what projection has maximum class separability while PCA finds what projection has maximal variances.  In addition, LDA is a supervised algorithm thus it requires supplying it with the y\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis(n_components=2)\n",
    "start = time()\n",
    "X_reduced = model.fit_transform(X, y)\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "plot_clusters(X_reduced, y)\n",
    "\n",
    "# Quite fast but lots of overlap...not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### PCA + TSNE\n",
    "\n",
    "# This is a common way to reduce the time of TSNE by first using PCA to scale down the data only to 95% variance ratio, and fit it to TSNE.  Most of the time, it can reduce the time by almost 50% sometimes\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
    "    (\"tsne\", TSNE(n_components=2, random_state=42)),\n",
    "])\n",
    "\n",
    "start = time()\n",
    "X_reduced = model.fit_transform(X)\n",
    "print(f\"Training took {time() - start:.2f}\")\n",
    "\n",
    "plot_clusters(X_reduced, y)\n",
    "\n",
    "# Nice! We speed up the TSNE by first applying PCA.  As we look at the visualization, it seems it is quite comparable with pure tsne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \n",
    "\n",
    "attempt to compare the followings:\n",
    "\n",
    " 1. SVM prediction with PCA of 0.99 explained variances\n",
    " 2. SVM prediction with PCA of 0.90 explained variances\n",
    "\n",
    "- you probably want to use grid search to find the best C and gamma\n",
    "- since image is non-linear, it may be wise to use rbf/poly kernel\n",
    "- since there are imbalanced set of target images, use class_weight='balanced'\n",
    "- for pca, since we are working with images, with many correlated pixels\n",
    "- it is useful to use whiten=True which will transform our covariance matrix to unit matrix (it is similar to standardizing your PCA)\n",
    "\n",
    "\n",
    "How much time differences between the two?\n",
    "\n",
    "How about the accuracy? (Hint: accuracy should be avoided in imbalanced dataset)\n",
    "\n",
    "Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Your code here\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70)\n",
    "\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "#splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#scale helps svm since without scaling, svm\n",
    "#tend to perform badly as it got influenced\n",
    "#by bigger features.  Thus it is best\n",
    "#to standardized the values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"=======Performing PCA========\")\n",
    "pca90 = PCA(n_components=0.9, whiten=True).fit(X_train)\n",
    "start = time()\n",
    "X_train_reduced90 = pca90.transform(X_train)\n",
    "X_test_reduced90 = pca90.transform(X_test)\n",
    "pca90faces = pca90.components_.reshape((X_train_reduced90.shape[1], h, w))\n",
    "print(f\"PCA 90 took {time() - start: .2f}s\")\n",
    "print(\"PCA 90 n_features: \", X_train_reduced90.shape[1])\n",
    "\n",
    "pca99 = PCA(n_components=0.99, whiten=True).fit(X_train)\n",
    "start = time()\n",
    "X_train_reduced99 = pca99.transform(X_train)\n",
    "X_test_reduced99 = pca99.transform(X_test)\n",
    "pca99faces = pca99.components_.reshape((X_train_reduced99.shape[1], h, w))\n",
    "\n",
    "print(f\"PCA 99 took {time() - start: .2f}s\")\n",
    "print(\"PCA 99 n_features: \", X_train_reduced99.shape[1])\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "print(\"=======Plotting images and their components========\")\n",
    "\n",
    "ax[0].imshow(X_train[0].reshape(h, w))\n",
    "ax[0].set_title(target_names[y[0]])\n",
    "\n",
    "ax[1].imshow(pca99faces[0].reshape((h, w)))\n",
    "ax[1].set_title(f\"Feature 0 - PCA 99\")\n",
    "\n",
    "ax[2].imshow(pca99faces[1].reshape((h, w)))\n",
    "ax[2].set_title(f\"Feature 1 - PCA 90\")\n",
    "\n",
    "#this is also known as eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def search_and_predict(X_train, X_test, y_train, y_test):\n",
    "    start = time()\n",
    "    #default kernel is rbf anyway\n",
    "    svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "    param_grid = {'C': [1, 10, 100, 1000],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]} \n",
    "    grid = GridSearchCV(svc, param_grid)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Best estimator: \", grid.best_estimator_)\n",
    "    print(f\"Search took {time() - start: .2f}\")\n",
    "    start = time()\n",
    "    pred = grid.predict(X_test)\n",
    "    print(f\"Prediction took {time() - start: .2f}\")\n",
    "    print(\"Classification report: \")\n",
    "    print(classification_report(y_test, pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"===PCA 99=====\")\n",
    "search_and_predict(X_train_reduced99, X_test_reduced99, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"===PCA 90=====\")\n",
    "search_and_predict(X_train_reduced90, X_test_reduced90, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a very imbalanced dataset (look at the support column), it would be a crime to judge the effectiveness using accuracy.  More useful metric is likely f1-score since we don't know whether we want to prioritize recall or precision.  Looking at the f1score mean across all class, the f1 scores best when n_component is around 0.9 (indeed, we can confirm with grid search below).  This informs us couple of things:\n",
    "\n",
    "1. Sometimes, dimensionality reduction in a way remove noise and feed the \"cleaner\" data to the model, in this case the SVC, and can increase the score\n",
    "2. Higher explained ratio, 0.99, may not necessarily result in higher accuracy than 0.9, for example.  The explanation is basically the same as 1, which is related to removing \"unnecessary information\".  But often, it may be worthwhle to put in the GridSearch and find the best dimension to reduce\n",
    "3. The speed gain from PCA 90 is incredible.  Not to mention it also increases the accuracy.  Thus it is sometimes worth exploring dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#improve code so we simply search the best\n",
    "#but it can take a long time!\n",
    "def search_and_predict(X_train, X_test, y_train, y_test):\n",
    "    start = time()\n",
    "    pca = PCA(n_components=0.5, whiten=True, random_state=42)\n",
    "    svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "    model = make_pipeline(pca, svc)\n",
    "    param_grid = {'svc__C': [1, 10, 100, 1000],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], \n",
    "            'pca__n_components': [0.1, 0.5, 0.7, 0.9, 0.99]} \n",
    "    grid = GridSearchCV(model, param_grid)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Best estimator: \", grid.best_estimator_)\n",
    "    print(f\"Search took {time() - start: .2f}\")\n",
    "    start = time()\n",
    "    pred = grid.predict(X_test)\n",
    "    print(f\"Prediction took {time() - start: .2f}\")\n",
    "    print(\"Classification report: \")\n",
    "    print(classification_report(y_test, pred, target_names=target_names))\n",
    "search_and_predict(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\n",
    "\n",
    "Modify the scratch code of PCA in our lecture:\n",
    "\n",
    "- Modify so instead of using np.linalg.eigh, let's replace it with scratch code using SVD approach\n",
    "- Convert it using kernel PCA, where we convert our X using rbf kernels.  You may want to see how to transfer your data to another space via https://en.wikipedia.org/wiki/Radial_basis_function_kernel and this http://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/#References. For those who are confused what is x and x' (prime), they are basically each sample, so you may want to first find the squared distances between each sample.  Apply your kernel PCA with a sklearn datasets make_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dfba24ab3275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#use the following data\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#elevation and #azimuth\n",
    "ax.view_init(5, -80)\n",
    "\n",
    "X, color = datasets.make_swiss_roll(n_samples=500, noise=0.3)\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "X, color = datasets.make_swiss_roll(n_samples=1500, noise=0.3)\n",
    "\n",
    "#see below cell for numpy way\n",
    "squared_distances = pdist(X, 'sqeuclidean')\n",
    "squared_distances_matrix = squareform(squared_distances)\n",
    "\n",
    "#make sure that each sample is subtracted with each sample\n",
    "assert X.shape[0], X.shape[0] == squared_distances_matrix.shape \n",
    "\n",
    "#let's say we have gamma 0.00001\n",
    "#Recap: smaller gamma gives you low bias and high variance\n",
    "#large gamma will give you higher bias and low variance\n",
    "#Basically, gamma is 1/(2*sigma^(2)) but we simply make it easier\n",
    "#thus if we choose very big sigma (high variance), then gamma will be small\n",
    "#hence smaller gamma gives you higher variance\n",
    "gamma = 0.00001\n",
    "\n",
    "#you should be able to use scipy version of exp too!\n",
    "rbf = np.exp(-gamma * squared_distances_matrix)\n",
    "print(\"Kernel shape: \", rbf.shape)\n",
    "\n",
    "#since we did not perform any centering yet, \n",
    "#we can use this formula K′=K−1_{N}K−K1_{N}+1_{N}K1_{N}\n",
    "#this 1_{n} is a N X N matrix with all values equal to 1/N\n",
    "n_samples = X.shape[0]\n",
    "one_over_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "rbf = rbf - (one_over_n @ rbf) - (rbf @ one_over_n) + (one_over_n @ rbf @ one_over_n)    \n",
    "    \n",
    "#rbf is essentially covariances in rbf space, so simply insert the rbf inside the eig func\n",
    "eigenvalues, eigenvectors = np.linalg.eig(rbf)\n",
    "\n",
    "# sorting eigenvalues from biggest to smallest\n",
    "ix = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "#hard code it\n",
    "n_components = 2\n",
    "\n",
    "if(n_components > eigenvalues.shape[0]):\n",
    "    raise Exception(\"You cannot have n_components more than number of features!\")\n",
    "else:\n",
    "    eigenvalues, eigenvectors = eigenvalues[ix], eigenvectors[:, ix]\n",
    "    #I did not have .T here since I will not be looping\n",
    "    eigenvalues, eigenvectors = eigenvalues[:n_components], eigenvectors[:,:n_components]\n",
    "        \n",
    "    #define how much variance is gained after each component\n",
    "    variance_explained_ratio = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "\n",
    "    #projected new vector\n",
    "    #instead of projecting the X to the eigenvectors\n",
    "    #we project the kernel onto the eigenvectors\n",
    "    projected_X = rbf @ eigenvectors\n",
    "    \n",
    "    #print all info\n",
    "    print(\"Variance explained_ratio: \", variance_explained_ratio) #first component got 97%\n",
    "    print(\"Eigenvalues: \", eigenvalues.shape)\n",
    "    print(\"Eigenvectors (column-wise): \", eigenvectors.shape)\n",
    "    print(\"Mean: \", mean)\n",
    "    print(\"Old X shape: \", X.shape)\n",
    "    print(\"Projected X shape: \", projected_X.shape)\n",
    "\n",
    "#projection\n",
    "plt.figure()\n",
    "plt.scatter(projected_X[:, 0], projected_X[:, 1], c=color,\n",
    "           cmap=\"rainbow\", edgecolor='k')\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.title(\"Projected X\")\n",
    "plt.axis('equal')\n",
    "\n",
    "#compare with sklearn kernel pca\n",
    "from sklearn.decomposition import KernelPCA\n",
    "model = KernelPCA(kernel=\"rbf\", n_components=2,\n",
    "                 gamma=0.000001)\n",
    "out = model.fit_transform(X)\n",
    "plt.figure()\n",
    "plt.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for those who do not understand what does pdist and squareform does\n",
    "test = np.arange(6).reshape((3, 2))\n",
    "print(\"Input data: \", test)\n",
    "\n",
    "sq = pdist(test, 'sqeuclidean')\n",
    "print(\"Square distance: \", sq)\n",
    "\n",
    "some_mat = squareform(sq)\n",
    "print(\"Matrix form: \", some_mat)\n",
    "\n",
    "#basically it is just (x - x')^2 along the columns (feature space)\n",
    "\n",
    "#numpy way\n",
    "print(\"Numpy: \", ((test[:, np.newaxis, :] - test[np.newaxis, :, :]) ** 2).sum(2))\n",
    "\n",
    "#sklearn way\n",
    "from sklearn import metrics\n",
    "m = metrics.pairwise_distances(test, test)\n",
    "print(\"Sklearn way: \", m**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\n",
    "\n",
    "Implement ISOmap from scratch. Reduced to 200 samples for peace of mind.\n",
    "<code>X, color = datasets.make_swiss_roll(n_samples=200, noise=0.3)</code>\n",
    "\n",
    "Since ISOmap is essential very similar to MDS but instead of a simple distance matrix, it feeds in a shortest path matrix instead, hence the geodesic distances.\n",
    "\n",
    "First, compute the distances.  Make sure it is euclidean for this step. Also make sure that the distance matrix should have values for only the nth nearest neighbors. Otherwise, values should be infinity. Infinity will be later on used by Floyd algorithm to determine the shortest paths. Last, treat this distance matrix as undirected graph, which means <code>distance\\[i\\]\\[j\\]</code> should be same as <code>distance\\[j\\]\\[i\\]</code>\n",
    "\n",
    "Second, input the distance matrix into Floyd algorithm. This part can be unfamiliar for those who did not study Data Structures and Algorithms.  \n",
    "Floyd Warshall algorithm is basically a all-pairs shortest-path algorithm. It is pretty straightforward algorithm in which you loop through all possible pair of vertices and their possible paths and keep on updating if shorter path is found. \n",
    "This is implemented based on pseudocode given in https://en.wikipedia.org/wiki/Floyd–Warshall_algorithm.\n",
    "For those who want to first get an intuition, watch this https://www.youtube.com/watch?v=4OQeCuLYj-4.\n",
    "Here is how the floyd algorithm can be implemented in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floydWarshall(distance_matrix):\n",
    "    #shorten the name for shorter code!\n",
    "    #I write the long name as param so you guys will know\n",
    "    #what is it that is being input to floyd\n",
    "    dist = distance_matrix\n",
    "    length = len(dist)\n",
    "    for k in range(length):\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                dist[i][j] = min(dist[i][j],dist[i][k] + dist[k][j])\n",
    "    return graph_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, you need to feed the graph matrix to the MDS algorithm. This MDS algorithm is quite straightforward as well.\n",
    "For simplicity, you can follow the steps written in https://en.wikipedia.org/wiki/Multidimensional_scaling in topic \"Steps of a classical MDS algorithm\"\n",
    "\n",
    "1. Squared the incoming matrix return my floyd\n",
    "\n",
    "2. Perform double centering which center the data, and remove asymmetrical distances by multiplying 0.5\n",
    "\n",
    "3. Use elg() to find eigenvalues and eigenvectors\n",
    "\n",
    "4. The new X = Em(Am)^1/2, where EM is eigenvectors and Am is the diagonal matrix of eigenvalues\n",
    "\n",
    "Phew, it may look hard, but it is always satisfying to understand the math behind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement ISOmap from scratch\n",
    "#reduced to 200 samples for peace of mind\n",
    "X, color = datasets.make_swiss_roll(n_samples=200, noise=0.3)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "\n",
    "#reduced to 300 samples for peace of mind\n",
    "X, color = datasets.make_swiss_roll(n_samples=200, noise=0.3)\n",
    "\n",
    "#copy the floyd warshall algorithm from above\n",
    "#you can actually use this as well:\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.floyd_warshall.html\n",
    "def floyd(distance_matrix):\n",
    "    #shorten the name for shorter code!\n",
    "    #I write the long name as param so you guys will know\n",
    "    #what is it that is being input to floyd. Does not hurt to\n",
    "    #make things more readable\n",
    "    dist = distance_matrix\n",
    "    length = len(dist)\n",
    "    for k in range(length):\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                dist[i][j] = min(dist[i][j],dist[i][k] + dist[k][j])\n",
    "    return dist\n",
    "\n",
    "#very similar to above but use euclidean \n",
    "#since Floyd uses that\n",
    "#Also, do not forget to only assign distance of nearest neighbors\n",
    "#to the distance matrix\n",
    "def make_dist_matrix(X, n_neighbors=10):\n",
    "    \n",
    "    #find distance matrix\n",
    "    dist_mat = squareform(pdist(X, 'euclidean'))\n",
    "    #make sure that each sample is subtracted with each sample\n",
    "    assert X.shape[0], X.shape[0] == dist_mat.shape \n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    dist = np.ones((n_samples, n_samples)) * np.inf\n",
    "    for i in range(n_samples):    \n",
    "        #get n nearest neighbors\n",
    "        index_arr = np.argsort(dist_mat[i])[:n_neighbors]\n",
    "        #put only n nearest neighbors distance into the matrix\n",
    "        #otherwise, let it be infinity\n",
    "        for j in index_arr:\n",
    "            dist[i][j] = dist_mat[i][j]\n",
    "            #i j is same as j i for undirected matrix\n",
    "            #clearly, mds or isomap does not care about direction\n",
    "            #it only cares about the neighbors, thus we\n",
    "            #need to treat this matrix as undirected graph\n",
    "            dist[j][i] = dist_mat[i][j]\n",
    "\n",
    "    return dist\n",
    "    \n",
    "#very similar to kernel pca (since rbf is kinda like gaussian, right?)\n",
    "def mds(graph_mat):\n",
    "    mat = graph_mat**2\n",
    "    n_samples = graph_mat.shape[0]\n",
    "\n",
    "    #we can center the data using \"centering matrix\"\n",
    "    #https://en.wikipedia.org/wiki/Centering_matrix\n",
    "    identity = np.identity(n_samples)\n",
    "    ones = np.ones((n_samples, n_samples))\n",
    "    \n",
    "    #C_{n} = I_{n} - 1/n * 11.T\n",
    "    centering_matrix = identity -  (1/n_samples) * ones \n",
    "    \n",
    "    #centering_mat @ mat removes the mean from each of n columns\n",
    "    #mat @ centering_mat removes the mean from each of m rows\n",
    "    #by doing both, both row and column means are equal to zero\n",
    "    #-0.5 is for \"double centering\" as specify in step 2 in the wikipedia\n",
    "    mat = -0.5 * (centering_matrix @ mat @ centering_matrix)\n",
    "    \n",
    "    #mat is essentially covariances in gaussian space (i.e., nearest neighbors)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(mat)\n",
    "\n",
    "    # sorting eigenvalues from biggest to smallest\n",
    "    ix = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "    #hard code it\n",
    "    n_components = 2\n",
    "\n",
    "    if(n_components > eigenvalues.shape[0]):\n",
    "        raise Exception(\"You cannot have n_components more than number of features!\")\n",
    "    elif(n_components > n_samples):\n",
    "        #https://stackoverflow.com/questions/51040075/why-sklearn-pca-needs-more-samples-than-new-featuresn-components\n",
    "        raise Exception(\"You also cannot have n_components more than number of samples!  You won't be able to find the eigenvalues and vectors!\")\n",
    "    else:\n",
    "        eigenvalues, eigenvectors = eigenvalues[ix], eigenvectors[:, ix]\n",
    "        eigenvalues, eigenvectors = eigenvalues[:n_components], eigenvectors[:,:n_components]\n",
    "\n",
    "        #define how much variance is gained after each component\n",
    "        variance_explained_ratio = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "\n",
    "        #projected new vector\n",
    "        #based on https://en.wikipedia.org/wiki/Multidimensional_scaling\n",
    "        #\"Steps of a classical MDS algorithm - Step 4\"\n",
    "        Em = eigenvectors\n",
    "        Am = np.diag(eigenvalues)\n",
    "        projected_X = Em @ np.sqrt(Am)\n",
    "\n",
    "        #print all info\n",
    "        print(\"Variance explained_ratio: \", variance_explained_ratio) #first component got 97%\n",
    "        print(\"Eigenvalues: \", eigenvalues.shape)\n",
    "        print(\"Eigenvectors (column-wise): \", eigenvectors.shape)\n",
    "        print(\"Mean: \", mean)\n",
    "        print(\"Old X shape: \", X.shape)\n",
    "        print(\"Projected X shape: \", projected_X.shape)\n",
    "        \n",
    "        return projected_X\n",
    "        \n",
    "#first, find the euclidean distance matrix\n",
    "#then, find the shortest path matrix based on floyd which\n",
    "#gives the geodesic distance\n",
    "#last, input into mds\n",
    "\n",
    "# projected_X = mds(shortest_path(make_dist_matrix(X, n_neighbors=10), method='FW', directed=False))\n",
    "projected_X = mds(floyd(make_dist_matrix(X, n_neighbors=10)))\n",
    "\n",
    "#projection\n",
    "plt.figure()\n",
    "plt.scatter(projected_X[:, 0], projected_X[:, 1], c=color,\n",
    "           cmap=\"rainbow\", edgecolor='k')\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.title(\"Projected X\")\n",
    "plt.axis('equal')\n",
    "\n",
    "#compare with sklearn isomap\n",
    "from sklearn.manifold import Isomap\n",
    "model = Isomap(n_neighbors=10, n_components=2)\n",
    "out = model.fit_transform(X)\n",
    "plt.figure()\n",
    "plt.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

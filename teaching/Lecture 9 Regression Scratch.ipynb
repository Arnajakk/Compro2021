{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type - Bunch\n",
    "#Bunch - dictionary of numpy data\n",
    "#boston.feature_names\n",
    "#print(boston)\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "X.shape #number of samples, number of features\n",
    "\n",
    "#====Step 1: Get your X and y in the right shape======\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in X is the same as number of rows in y\n",
    "#because so we have yhat for all y\n",
    "assert m == y.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to standardize my data so that mean is 0, variance is 1\n",
    "#average across each feature, NOT across each sample\n",
    "#Why we need to standardize\n",
    "#Because standardizing usually allows us to reach convergence faster\n",
    "#Why -> because the values are within smaller range\n",
    "#Thus, the gradients are also within limited range, and NOT go crazy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#1. StandardScaler.fit(X)  #this scaler (or self) knows the mean and std so now\n",
    "# it knows how to transform data\n",
    "#2  X = StandardScaler.transform(X)  #not in place; will return something\n",
    "\n",
    "#1. StandardScaler.fit_transform(X) -> 1 and 2 sequentially\n",
    "\n",
    "#create an object of StandardScaler\n",
    "#StandardScaler is a class\n",
    "#scaler is called instance/object\n",
    "\n",
    "#====Step 2: ALMOST always, feature scale your data using normalization or standardization\n",
    "#If you assume your data is gaussian, use standardization, otherwise, you do the normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====Step 3: Train test split your data \n",
    "\n",
    "#what is the appropriate size for test data\n",
    "#70/30 (small dataset); 80/20 (medium dataset); 90/10 (large dataset);\n",
    "#why large dataset, can set test size to 10, because\n",
    "#10% of large dataset is already enough for testing accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closed form\n",
    "#How to get Closed Form\n",
    "#Simple; Set the d(cost function) = 0\n",
    "#And find the \\theta that satisfy the equation\n",
    "#When we can do such a thing in which we set the d(cost function) = 0\n",
    "#--->When its strictly concave, or strictly convex\n",
    "#----> They have only one local maximum (concave), minimum (convex)\n",
    "#=====Step 4: Define your procedure to find theta ===\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#run it, and return me the theta\n",
    "#which one do first DOES NOT MATTER\n",
    "#But don't flip y before X^T for example\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the shape of X they want\n",
    "#(number of samples, number of features) --> correct shape\n",
    "# for closed form formula\n",
    "#How about the intercept\n",
    "#w0 is OUR intercept\n",
    "#what is the common shape of w -->(n+1, )\n",
    "#What is the shape of 1 --->(m, 1)\n",
    "#X = [1 2 3     @  [w0\n",
    "#     1 4 6         w1\n",
    "#     1 9 1         w2 \n",
    "#     1 10 2 ] \n",
    "\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.7281486 , -1.03784848,  1.10085393,  0.05958242,  0.98754043,\n",
       "       -2.18106046,  3.13039422,  0.12442295, -3.36474438,  2.76242277,\n",
       "       -1.99224799, -1.89946081,  0.88911465, -3.71947443])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====Step 5: Find the theta/weights/coefficients/beta=====\n",
    "w = closed_form(X_train, y_train)\n",
    "w  #<------somehow is our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====Step 6: Compute the accuracy/loss\n",
    "\n",
    "#6.1 predict --> \\theta^T x\n",
    "yhat = X_test @ w #==> X (m, n+1)  @ (n+1, ) w ==> (m, ) y\n",
    "\n",
    "#if I want to compare yhat and y, I need to make sure they are the same shape\n",
    "assert y_test.shape == yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2. ---> get the errors\n",
    "errors = ((y_test - yhat)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456.2815160721893\n"
     ]
    }
   ],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  22.786395077682954\n",
      "Stop at iteration:  741\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Prepare your data\n",
    "#X_train, X_test have intercepts that are being concatenated to the data\n",
    "#[1, features\n",
    "# 1, features....]\n",
    "\n",
    "#making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "#initialize our w\n",
    "#We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "#intercept\n",
    "#w = theta/beta/coefficients\n",
    "w = np.zeros(X_train.shape[1])\n",
    "\n",
    "#define the learning rate\n",
    "#later on, you gonna know that it should be better to make it slowly decreasing\n",
    "#once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "#define our max_iter\n",
    "#typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 1000\n",
    "\n",
    "loss_old = 10000\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def get_yhat(X, w):\n",
    "    return X @ w\n",
    "\n",
    "def get_mse(yhat, y):\n",
    "    return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "def delta_loss(new, old, tol):\n",
    "    return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "#define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    #1. yhat = X @ w\n",
    "    #prediction\n",
    "    #yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = get_yhat(X_train, w)\n",
    "\n",
    "    #2. error = yhat - y_train\n",
    "    #error for use to calculate gradients\n",
    "    #error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "\n",
    "    #2.1 early stopping\n",
    "    #so we don't go through all max_iter iterations\n",
    "    # )yi_hat - yi )^2 / m  <--- mse\n",
    "    #loss_new (scalar) = ((m, ) - (m, ) **2 / m).sum()\n",
    "    loss_new = get_mse(yhat, y_train)\n",
    "    if delta_loss(loss_new, loss_old, tol):  #np.allclose\n",
    "        iter_stop = i\n",
    "        break\n",
    "    loss_old = loss_new\n",
    "\n",
    "    #3. grad = X.T @ error\n",
    "    #grad (n, ) = (n, m) @ (m, )\n",
    "    #grad for each feature j\n",
    "    grad = gradient(X_train, error)\n",
    "\n",
    "    #4. w = w - alpha * grad\n",
    "    #update w\n",
    "    #w (n, ) = (n, ) - scalar * (n, )\n",
    "    w = w - alpha * grad\n",
    "\n",
    "\n",
    "#we got our lovely w\n",
    "#now it's time to check our accuracy\n",
    "#1. Make prediction\n",
    "yhat = get_yhat(X_test, w)\n",
    "\n",
    "#2. Calculate mean squared errors\n",
    "mse = get_mse(yhat, y_test)\n",
    "\n",
    "#print the mse\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iteration: \", iter_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  26.42954342037887\n",
      "Stop at iteration:  0\n"
     ]
    }
   ],
   "source": [
    "#Stochastic version\n",
    "\n",
    "def h_theta_x(X, w):\n",
    "    '''\n",
    "    Input:\n",
    "        X shape (m, n)\n",
    "        w shape (n, )\n",
    "    Returns: \n",
    "        (m, )\n",
    "    '''\n",
    "    return X @ w\n",
    "\n",
    "def get_mse(yhat, y):\n",
    "    return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "def get_singe_mse(yhat, y):\n",
    "    #yhat (1, ) - (1, ) ** 2\n",
    "    return (yhat - y)**2\n",
    "\n",
    "def delta_loss(new, old, tol):\n",
    "    return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "#initialize our w\n",
    "#We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "#intercept\n",
    "#w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "#define the learning rate\n",
    "#later on, you gonna know that it should be better to make it slowly decreasing\n",
    "#once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.01\n",
    "loss_old = 10000\n",
    "tol = 0.0001\n",
    "iter_stop = 0\n",
    "max_epochs = 10000\n",
    "\n",
    "\n",
    "#define your for loop\n",
    "for epoch in range(max_epochs):  #max_iter is the same as epochs\n",
    "\n",
    "    #we have indices for all samples\n",
    "    i = np.random.randint(X_train.shape[0])\n",
    "    \n",
    "    #1. yhat = X_i @ w\n",
    "    #X_i (1, n)\n",
    "    X_i = X_train[i, :].reshape(1, -1)\n",
    "\n",
    "    #prediction\n",
    "    #yhat (1, ) = (1, n) @ (n, )\n",
    "    yhat = h_theta_x(X_i, theta)\n",
    "\n",
    "    #2. error = yhat - y_i\n",
    "    #y_i (1, )\n",
    "    y_i = y_train[i]\n",
    "    #error for use to calculate gradients\n",
    "    #error (1, ) = (1, ) - (1, )\n",
    "    error = yhat - y_i\n",
    "\n",
    "    #2.1 early stopping\n",
    "    #so we don't go through all max_iter iterations\n",
    "    # (yi_hat - yi )^2 / m  <--- mse\n",
    "    #loss_new (scalar) = ((m, ) - (m, ) **2 / m).sum()\n",
    "    loss_new = get_singe_mse(yhat, y_i)\n",
    "    if delta_loss(loss_new, loss_old, tol):  #np.allclose\n",
    "        iter_stop = epoch\n",
    "        break\n",
    "    loss_old = loss_new\n",
    "\n",
    "    #3. grad = X.T @ error\n",
    "    #grad (n, ) = (n, 1) @ (1, )\n",
    "    #grad for each feature j\n",
    "    grad = gradient(X_i, error)\n",
    "\n",
    "    #4. w = w - alpha * grad\n",
    "    #update w\n",
    "    #w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "\n",
    "\n",
    "#we got our lovely w\n",
    "#now it's time to check our accuracy\n",
    "#1. Make prediction\n",
    "yhat = h_theta_x(X_test, theta)\n",
    "\n",
    "#2. Calculate mean squared errors\n",
    "mse = get_mse(yhat, y_test)\n",
    "\n",
    "#print the mse\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iteration: \", iter_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

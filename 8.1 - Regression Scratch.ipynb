{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some boston data as our regression case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type - Bunch\n",
    "#Bunch - dictionary of numpy data\n",
    "#boston.feature_names\n",
    "#print(boston)\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare your data\n",
    "\n",
    "#### 1.1 Get your X and y in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "X.shape #number of samples, number of features\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in X is the same as number of rows in y\n",
    "#because so we have yhat for all y\n",
    "assert m == y.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Feature scale your data to reach faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to standardize my data so that mean is 0, variance is 1\n",
    "#average across each feature, NOT across each sample\n",
    "#Why we need to standardize\n",
    "#Because standardizing usually allows us to reach convergence faster\n",
    "#Why -> because the values are within smaller range\n",
    "#Thus, the gradients are also within limited range, and NOT go crazy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#1. StandardScaler.fit(X)  #this scaler (or self) knows the mean and std so now\n",
    "# it knows how to transform data\n",
    "#2  X = StandardScaler.transform(X)  #not in place; will return something\n",
    "\n",
    "#1. StandardScaler.fit_transform(X) -> 1 and 2 sequentially\n",
    "\n",
    "#create an object of StandardScaler\n",
    "#StandardScaler is a class\n",
    "#scaler is called instance/object\n",
    "\n",
    "#ALMOST always, feature scale your data using normalization or standardization\n",
    "#If you assume your data is gaussian, use standardization, otherwise, you do the normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train test split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the appropriate size for test data\n",
    "#70/30 (small dataset); 80/20 (medium dataset); 90/10 (large dataset);\n",
    "#why large dataset, can set test size to 10, because\n",
    "#10% of large dataset is already enough for testing accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Add intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the shape of X they want\n",
    "#(number of samples, number of features) --> correct shape\n",
    "# for closed form formula\n",
    "#How about the intercept\n",
    "#w0 is OUR intercept\n",
    "#what is the shape of w -->(n+1, )\n",
    "#What is the shape of intercept --->(m, 1)\n",
    "#X = [1 2 3     @  [w0\n",
    "#     1 4 6         w1\n",
    "#     1 9 1         w2 \n",
    "#     1 10 2 ] \n",
    "\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Feature Engineering (optional)\n",
    "\n",
    "It is sometimes useful to engineer new features (e.g., polynomial, kernels) so to create some non-linear relationships with your target.\n",
    "\n",
    "Here we gonna skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: Closed Form\n",
    "\n",
    "The closed form is a normal equations derived from setting the derivatives = 0.  By performing only some inverse operations and matrix multiplication, we will be able to get the theta.\n",
    "\n",
    "When closed form is available, is doable (can be inversed - can use pseudoinverse), and with not many features (i.e., inverse can be slow), it is recommended to always use closed form.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closed form\n",
    "#How to get Closed Form\n",
    "#Simple; Set the d(cost function) = 0\n",
    "#And find the \\theta that satisfy the equation\n",
    "#When we can do such a thing in which we set the d(cost function) = 0\n",
    "#--->When its strictly concave, or strictly convex\n",
    "#----> They have only one local maximum (concave), minimum (convex)\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#run it, and return me the theta\n",
    "#which one do first DOES NOT MATTER\n",
    "#But don't flip y before X^T for example\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.64917743, -1.05547574,  1.22332735, -0.1144123 ,  0.75230475,\n",
       "       -2.03959642,  2.69724278, -0.06751152, -3.528664  ,  2.94806347,\n",
       "       -2.10031456, -2.15895157,  0.88454597, -3.93938543])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's use the closed_form to find the theta\n",
    "theta = closed_form(X_train, y_train)\n",
    "theta  #<------this is our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the accuracy/loss\n",
    "\n",
    "#6.1 predict --> \\theta^T x\n",
    "yhat = X_test @ theta #==> X (m, n+1)  @ (n+1, ) w ==> (m, ) y\n",
    "\n",
    "#if I want to compare yhat and y, I need to make sure they are the same shape\n",
    "assert y_test.shape == yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the errors\n",
    "errors = ((y_test - yhat)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2710.09911982039\n"
     ]
    }
   ],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2: Batch Gradient Descent\n",
    "\n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - X and y in the right shape\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict and calculate the loss\n",
    "    - The loss function is the mean squared error\n",
    "    $$J = \\frac{\\Sigma_{i=1}^{m}(h-y)^2}{m}$$\n",
    "    where h is simply\n",
    "    $$ h = \\theta^Tx $$\n",
    "3. Calculate the gradient based on the loss\n",
    "    - The gradient of the loss function is\n",
    "    $$\\frac{\\partial J}{\\partial \\theta_j} = \\Sigma_{i=1}^{m}(h^{(i)}-y^{(i)})x_j$$\n",
    "4. Update the theta with this update rule\n",
    "    $$\\theta_j := \\theta_j - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "    where $\\alpha$ is a typical learning rate range between 0 and 1\n",
    "5. Loop 2-4 until max_iter is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "#Step 1: Prepare your data\n",
    "#X_train, X_test have intercepts that are being concatenated to the data\n",
    "#[1, features\n",
    "# 1, features....]\n",
    "\n",
    "#making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "#initialize our w\n",
    "#We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "#intercept\n",
    "#w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "#define the learning rate\n",
    "#later on, you gonna know that it should be better to make it slowly decreasing\n",
    "#once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.0001\n",
    "\n",
    "#define our max_iter\n",
    "#typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 1000\n",
    "\n",
    "loss_old = 10000\n",
    "\n",
    "tol = 0.0001\n",
    "\n",
    "iter_stop = 0\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "def delta_loss(new, old, tol):\n",
    "    return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "start = time()\n",
    "\n",
    "#define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    #1. yhat = X @ w\n",
    "    #prediction\n",
    "    #yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = h_theta(X_train, theta)\n",
    "\n",
    "    #2. error = yhat - y_train\n",
    "    #error for use to calculate gradients\n",
    "    #error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "\n",
    "    #2.1 early stopping\n",
    "    #so we don't go through all max_iter iterations\n",
    "    # )yi_hat - yi )^2 / m  <--- mse\n",
    "    #loss_new (scalar) = ((m, ) - (m, ) **2 / m).sum()\n",
    "    loss_new = mse(yhat, y_train)\n",
    "    if delta_loss(loss_new, loss_old, tol):  #np.allclose\n",
    "        iter_stop = i\n",
    "        break\n",
    "    loss_old = loss_new\n",
    "\n",
    "    #3. grad = X.T @ error\n",
    "    #grad (n, ) = (n, m) @ (m, )\n",
    "    #grad for each feature j\n",
    "    grad = gradient(X_train, error)\n",
    "\n",
    "    #4. w = w - alpha * grad\n",
    "    #update w\n",
    "    #w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  17.819515132071835\n",
      "Stop at iteration:  797\n",
      "Time used:  0.017093181610107422\n"
     ]
    }
   ],
   "source": [
    "#we got our lovely w\n",
    "#now it's time to check our accuracy\n",
    "#1. Make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "#2. Calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "#print the mse\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iteration: \", iter_stop)\n",
    "print(\"Time used: \", time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3: Stochastic Gradient Descent\n",
    "\n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - X and y in the right shape\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict and calculate the loss\n",
    "3. Calculate the gradient based on the loss\n",
    "    - **This differs from batch gradient descent that it only uses one sample to estimate the loss and gradient**\n",
    "        $$\\frac{\\partial J}{\\partial \\theta_j} = (h^{(i)}-y^{(i)})x_j$$\n",
    "    where i is some random number\n",
    "4. Update the theta\n",
    "5. Loop 2-4 until max_iter is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic version\n",
    "\n",
    "def h_theta(X, w):\n",
    "    '''\n",
    "    Input:\n",
    "        X shape (m, n)\n",
    "        w shape (n, )\n",
    "    Returns: \n",
    "        (m, )\n",
    "    '''\n",
    "    return X @ w\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2 / yhat.shape[0]).sum()\n",
    "\n",
    "def singe_mse(yhat, y):\n",
    "    #yhat (1, ) - (1, ) ** 2\n",
    "    return (yhat - y)**2\n",
    "\n",
    "def delta_loss(new, old, tol):\n",
    "    return np.abs(loss_new - loss_old) < tol\n",
    "\n",
    "def gradient(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "#initialize our w\n",
    "#We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "#intercept\n",
    "#w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "#define the learning rate\n",
    "#later on, you gonna know that it should be better to make it slowly decreasing\n",
    "#once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "alpha = 0.01\n",
    "loss_old = 10000\n",
    "tol = 0.0001\n",
    "iter_stop = 0\n",
    "max_epochs = 10000\n",
    "\n",
    "start = time()\n",
    "\n",
    "#define your for loop\n",
    "for epoch in range(max_epochs):  #max_iter is the same as epochs\n",
    "\n",
    "    #we have indices for all samples\n",
    "    i = np.random.randint(X_train.shape[0])\n",
    "    \n",
    "    #1. yhat = X_i @ w\n",
    "    #X_i (1, n)\n",
    "    X_i = X_train[i, :].reshape(1, -1)\n",
    "\n",
    "    #prediction\n",
    "    #yhat (1, ) = (1, n) @ (n, )\n",
    "    yhat = h_theta(X_i, theta)\n",
    "\n",
    "    #2. error = yhat - y_i\n",
    "    #y_i (1, )\n",
    "    y_i = y_train[i]\n",
    "    #error for use to calculate gradients\n",
    "    #error (1, ) = (1, ) - (1, )\n",
    "    error = yhat - y_i\n",
    "\n",
    "    #2.1 early stopping\n",
    "    #so we don't go through all max_iter iterations\n",
    "    # (yi_hat - yi )^2 / m  <--- mse\n",
    "    #loss_new (scalar) = ((m, ) - (m, ) **2 / m).sum()\n",
    "    loss_new = singe_mse(yhat, y_i)\n",
    "    if delta_loss(loss_new, loss_old, tol):  #np.allclose\n",
    "        iter_stop = epoch\n",
    "        break\n",
    "    loss_old = loss_new\n",
    "\n",
    "    #3. grad = X.T @ error\n",
    "    #grad (n, ) = (n, 1) @ (1, )\n",
    "    #grad for each feature j\n",
    "    grad = gradient(X_i, error)\n",
    "\n",
    "    #4. w = w - alpha * grad\n",
    "    #update w\n",
    "    #w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - alpha * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  19.677024613338126\n",
      "Stop at iteration:  0\n",
      "Time used:  0.2084660530090332\n"
     ]
    }
   ],
   "source": [
    "#we got our lovely w\n",
    "#now it's time to check our accuracy\n",
    "#1. Make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "#2. Calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "#print the mse\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Stop at iteration: \", iter_stop)\n",
    "print(\"Time used: \", time_taken)\n",
    "\n",
    "#the time taken is strangely higher than gradient descent\n",
    "#perhaps using stochastic with decay learning rate may give us\n",
    "#a better chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 4: Mini-Batch Gradient Descent\n",
    "\n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - X and y in the right shape\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict and calculate the loss\n",
    "3. Calculate the gradient based on the loss\n",
    "    - **This differs from batch gradient descent that it only uses a subset of samples to estimate the loss and gradient**\n",
    "        $$\\frac{\\partial J}{\\partial \\theta_j} = \\Sigma_{i=start}^{batch}(h^{(i)}-y^{(i)})x_j$$\n",
    "    where start is a randomized number within the range of $m$ and batch is a predefined batch size, typically around 100 to 500\n",
    "4. Update the theta\n",
    "5. Loop 2-4 until max_iter is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol\n",
    "\n",
    "I will not implement this, but leave to your exercise.  Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

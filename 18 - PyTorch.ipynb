{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 18. PyTorch\n",
    "\n",
    "- [WEIDMAN] Ch7\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "Here we introduce PyTorch, an increasingly popular neural network framework based on **automatic differentiation**, which we introduced at the beginning of previous chapter.\n",
    "\n",
    "As in the rest of the book, we’ll write our code in a way that maps to the mental models of how neural networks work, writing classes for Layers, Trainers, and so on. In doing so, we won’t be writing our code in line with common PyTorch practices, but I recommend you to watch the 60-min blitz to understand different features of pytorch (https://pytorch.org/tutorials/).  Pytorch is really cool and simple!  For example, there is a function <code>tensor.to(cuda)</code> so you can use the GPU to run any model which is much faster.  I highly recommend checking it out.\n",
    "\n",
    "### Basics\n",
    "\n",
    "1. Basic autograd example 1               \n",
    "2. Basic autograd example 2               \n",
    "3. Loading data from numpy                \n",
    "4. Input pipline                          \n",
    "5. Save and load model \n",
    "\n",
    "#### 1. Basic autograd example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x:  tensor(2.)\n",
      "Gradient of w:  tensor(1.)\n",
      "Gradient of b:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Create tensors.\n",
    "# only tensors of floating point dtype can get gradient\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients\n",
    "# Pytorch tensor can automatically compute the derivative\n",
    "# of the parameters in respect to loss\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(\"Gradient of x: \", x.grad)    # x.grad = 2 \n",
    "print(\"Gradient of w: \", w.grad)    # w.grad = 1 \n",
    "print(\"Gradient of b: \", b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Basic autograd example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.5373,  0.3946, -0.2021],\n",
      "        [ 0.0053,  0.1415, -0.4333]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.1422, -0.0619], requires_grad=True)\n",
      "loss:  1.4837201833724976\n",
      "dL/dw:  tensor([[ 0.2383,  0.2634, -0.8096],\n",
      "        [ 0.1286,  0.2615, -0.3749]])\n",
      "dL/db:  tensor([-0.3221,  0.0213])\n",
      "loss after 1 step optimization:  1.4726998805999756\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Retrieve batch of data\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print(images.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'models/model.ckpt')\n",
    "model = torch.load('models/model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'models/params.ckpt')\n",
    "resnet.load_state_dict(torch.load('models/params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\">\n",
    "\n",
    "n a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$yield_{apple}  = w_{11} * temp + w_{12} * rainfall + w_{13} * humidity + b_{1}$$\n",
    "\n",
    "$$yield_{orange} = w_{21} * temp + w_{22} * rainfall + w_{23} * humidity + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> by looking at the training data, to make accurate predictions for new data (i.e. to predict the yields for apples and oranges in a new region using the average temperature, rainfall and humidity). This is done by adjusting the weights slightly many times to make better predictions, using an optimization technique called gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs.size())\n",
    "print(targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader\n",
    "\n",
    "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation (<code>[0:3]</code> in the above code). It returns a tuple (or pair), in which the first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a <code>DataLoader</code>, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader is typically used in a for-in loop. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 22.,  37.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.],\n",
      "        [119., 133.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data, with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, which can lead to faster reduction in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "\n",
    "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0305,  0.4500,  0.3552],\n",
      "        [-0.0460, -0.3471,  0.3826]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([ 0.4901, -0.4545], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(3, 2)  #nn.Linear assume this shape (in_features, out_features)\n",
    "print(model.weight)\n",
    "print(model.weight.size()) # (out_features, in_features)\n",
    "print(model.bias)\n",
    "print(model.bias.size()) #(out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our model is simply a function that performs a matrix multiplication of the <code>inputs</code> and the weights <code>w</code> and adds the bias <code>b</code> (for each observation)\n",
    "\n",
    "<img src = \"figures/dot.png\" width=\"400\">\n",
    "\n",
    "PyTorch models also have a helpful <code>.parameters</code> method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0305,  0.4500,  0.3552],\n",
       "         [-0.0460, -0.3471,  0.3826]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4901, -0.4545], requires_grad=True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())  #model.param returns a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the <code>model(tensor)</code> API to perform a forward-pass that generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48.1414, -10.6209],\n",
       "        [ 65.6002, -10.7049],\n",
       "        [ 84.0460, -28.7849],\n",
       "        [ 36.0949,  -5.9195],\n",
       "        [ 70.6605, -10.1742],\n",
       "        [ 48.1414, -10.6209],\n",
       "        [ 65.6002, -10.7049],\n",
       "        [ 84.0460, -28.7849],\n",
       "        [ 36.0949,  -5.9195],\n",
       "        [ 70.6605, -10.1742],\n",
       "        [ 48.1414, -10.6209],\n",
       "        [ 65.6002, -10.7049],\n",
       "        [ 84.0460, -28.7849],\n",
       "        [ 36.0949,  -5.9195],\n",
       "        [ 70.6605, -10.1742]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "The <code>nn</code> module contains a lot of useful loss function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error_loss = nn.MSELoss()\n",
    "softmax_cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6644.5342, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error_loss(preds, targets)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "*Learning rate and momentum**\n",
    "\n",
    "Instead of manually manipulating the model's weights & biases using gradients, we can use <code>torch.optim</code> API.  We can use <code>optim.SGD</code> to perform stochastic gradient descent where samples are selected in batches (often with random shuffling) instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a bit about other techniques we have used before and how they look like in PyTorch.  Since this is simple linear regression problem, we only use learning rate and momentum, but it's worth to talk it here and perhaps we can see the code in the neural network section\n",
    "\n",
    "*Weight initialization*\n",
    "\n",
    "We don’t need to worry about weight initialization at all: the weights in most PyTorch operations involving parameters, including nn.Linear, are automatically scaled based on the size of the layer.\n",
    "\n",
    "*Dropout*\n",
    "\n",
    "Dropout is similarly easy. Just as PyTorch has a built-in Module <code>nn.Linear(n_in, n_out)</code> that computes the operations of a Dense layer from before, the Module nn.Dropout(dropout_prob) implements the Dropout operation, with the caveat that the probability passed in is by default the probability of dropping a given neuron.\n",
    "\n",
    "*Learning rate decay*\n",
    "\n",
    "PyTorch has an <code>lr_scheduler</code> class that can be used to decay the learning rate over the epochs. The key import you need to get started is <code>from torch.optim import lr_scheduler</code>   We shall use it in the latter section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

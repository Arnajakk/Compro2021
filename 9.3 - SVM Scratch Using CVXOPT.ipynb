{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Credits: http://cvxopt.org\n",
    "\n",
    "Let's work on both the hard margin and soft margin SVM algorithm in Python using the well known **CVXOPT** library. While the algorithm in its mathematical form is rather straightfoward, its implementation in matrix form using the CVXOPT API can be challenging at first. This notebook will show the steps required to derive the appropriate vectorized notation as well as the inputs needed for the API.\n",
    "\n",
    "#### Notations\n",
    "\n",
    "#### Primal Problem\n",
    "\n",
    "Recall that our primal, optimization problem is of the form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\min_{w, b} f(w,b) & = \\min_{w, b}  \\  \\frac{1}{2} ||w||^2\n",
    "\t\\\\\n",
    "\ts.t. \\ \\  g_i(w,b) &= - y^{(i)} (w^T x^{(i)} + b) + 1 = 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Lagrange Method\n",
    "\n",
    "The method of Lagrange multipliers allows us to turn a constrained optimization problem into an unconstrained one of the form:\n",
    "\n",
    "$$\\mathcal{L}(w, b, \\alpha) =   \\frac{1}{2} ||w||^2 - \\sum_i^m \\alpha_i [y^{(i)} (w^T x^{(i)} + b) - 1]$$\n",
    "\n",
    "Where $\\mathcal{L}(w, b, \\alpha)$ is called the Lagrangian and $\\alpha_i$ are called the Lagrangian multipliers.\n",
    "\n",
    "Our primal optimization problem with the Lagrangian becomes the following:\n",
    "\n",
    "$$\\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "#### Dual Problem\n",
    "\n",
    "This is the idea of turning primal problem into dual problem by acknowledging that this is roughly the same:\n",
    "\n",
    "$$\\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right) =   \\max_\\alpha \\left( \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "This allows us to take the partial derivatives of $\\mathcal{L}(w, b, \\alpha)$ with respect to $w$ and $b$\n",
    ", equate to zero and then plug the results back into the original equation of the Lagrangian, hence generating an equivalent dual optimization problem of the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\max_{\\alpha} \\min_{w,b} \\mathcal{L}(w,b,\\alpha)\n",
    "\t\\\\\n",
    "\t& \\max_{\\alpha} \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}> \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\alpha_i \\geq 0 \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\sum_i^m \\alpha_i y^{(i)} = 0\t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Duality and KTT\n",
    "\n",
    "Karush Kuhn Tucker (KTT) conditions allow us to solve the dual problem instead of the primal one, while ensuring that the optimal solution is the same. In our case the conditions are the following:\n",
    "\n",
    "- The primal objective and inequality constraint functions must be convex\n",
    "- The equality constraint function must be affine\n",
    "- The constraints must be strictly feasible\n",
    "\n",
    "Then there exists $w^*$, $\\alpha^*$ which are solutions to the primal and dual problems. Moreover, the parameters $w^*$, $\\alpha^*$ satisfy the KTT conditions below:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\frac{\\partial}{\\partial w_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(A)\n",
    "\t\\\\\n",
    "\t&\\frac{\\partial}{\\partial \\beta_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(B)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* g_i(w^*) = 0 &(C)\n",
    "\t\\\\\n",
    "\t&g_i(w^*) \\leq 0  &(D)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* \\geq 0 &(E)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Moreover, if some $w^*$, $\\alpha^*$ satisfy the KTT solutions then they are also solution to the primal and dual problem.\n",
    "\n",
    "Equation $(C)$ above is of particular importance and is called the *dual complementarity* condition. It implies that if $\\alpha_i^* > 0$ then $g_i(w^*) = 0$ which means that the constraint $g_i(w^*) \\leq 0$ is active, i.e., it holds with equality rather than inequality.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\min \\frac{1}{2} x^TPx + q^Tx\n",
    "    \\\\\n",
    "     s.t. \\ & \\ Gx \\leq h \n",
    "    \\\\\n",
    "    & \\ Ax = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}>\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\max_{\\alpha} \\sum_i^m \\alpha_i  - \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha\n",
    "    \\\\\n",
    "     s.t. & \\ \\alpha_i \\geq 0 \n",
    "    \\\\\n",
    "    &  \\ \\sum_i^m \\alpha_i y^{(i)} = 0  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\min_{\\alpha}  \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha - 1^T \\alpha\n",
    "    \\\\\n",
    "    & s.t. \\ - \\alpha_i \\leq 0 \n",
    "    \\\\\n",
    "    & s.t. \\ y^T \\alpha = 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} x_1^{(1)} & x_2^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} \\end{bmatrix} \\ \\ \\ y = \\begin{bmatrix} y^{(1)}  \\\\ y^{(2)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' = \\begin{bmatrix} x^{(1)}_1 y^{(1)} & x^{(1)}_2y^{(1)} \\\\\n",
    "x^{(2)}_1y^{(2)} & x^{(2)}_2y^{(2)} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

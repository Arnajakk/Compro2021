{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Bagging ensembles of Decision Trees from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "1. Load any data you would like to apply\n",
    "2. Make sure to subsample *with* replacement.  I.e., \n",
    "   we should allow the same training instance to be subsampled \n",
    "   for the same predictor.  For example, X[5] can appear multiple\n",
    "   times in the nth predictor (it may not even appear at all!)\n",
    "3. Once all predictors are trained, the bagging ensemble can make a prediction\n",
    "   by simply aggregating, commonly using mode for classification and\n",
    "   average for regression\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement AdaBoost for binary classification from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "1. Load any binary classification problem you would like to apply\n",
    "2. Given binary classification, AdaBoost expects input as -1 and 1\n",
    "3. Once all predictors have been trained, the final predictions are\n",
    "simply the np.sign of a_j * y_pred.  This works because if more\n",
    "classifiers say that it is positive, then the sum will turn out\n",
    "to be positive, thus we can simply look at the np.sign to know\n",
    "the class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Gradient Boosting from scratch\n",
    "#To add to the challenge, we shall create our algorithm that\n",
    "#works for both binary classification and regression\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "1. Load one binary classification and one regression problem you would like to apply\n",
    "2. To make our algorithm work for both regression and binary classification\n",
    "   We can perform this via a params called 'loss'\n",
    "   if loss is defined as \"mse\", then its a regression problem and\n",
    "   the residual errors shall be simply calculated as y - f(x)\n",
    "   if loss is defined as \"logistic\", then its a classification problem\n",
    "   and the residual errors shall be calculated as y - sigmoid(x)\n",
    "   the reason of using sigmoid is obvious, since we want to create\n",
    "   a mapping of x to something between 0 and 1 so it can be compared\n",
    "   to y, thus we use sigmoid\n",
    "3. You may wonder what classifier or regressor to use for this\n",
    "   problem.  Luckily, if we define the loss function with sigmoid,\n",
    "   we can simply use regressor as our estimator, since we already\n",
    "   define sigmoid function that map any continuous value to values \n",
    "   between 0 and 1.  The only thing you have to do is to \n",
    "   create a tolerance function mapping >0.5 to 1 otherwise 0\n",
    "4. When finding the loss, it is important to calculate the loss\n",
    "   based on the total errors made by all models you have trained\n",
    "   up to nth iteration\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        19\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.97      0.95      0.96        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "n_estimators = 5\n",
    "boostrap_ratio = 0.1\n",
    "tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#sample size for each predictor\n",
    "sample_size = int(boostrap_ratio * len(X_train))\n",
    "\n",
    "#2 for x and y\n",
    "xsamples = []\n",
    "ysamples = []\n",
    "\n",
    "#subsamples for each model\n",
    "for _ in range(n_estimators):\n",
    "    xsample = []\n",
    "    ysample = []\n",
    "    ##sampling with replacement; i.e., sample can occur more than once\n",
    "    #for the same predictor\n",
    "    for _ in range(sample_size):\n",
    "        idx = random.randrange(X_train.shape[0])\n",
    "        xsample.append(X_train[idx])\n",
    "        ysample.append(y_train[idx])\n",
    "    xsamples.append(xsample)\n",
    "    ysamples.append(ysample)\n",
    "\n",
    "#convert to numpy for easier manipulation\n",
    "#usually if we start with list.append and later convert to np\n",
    "#is faster than using np.empty\n",
    "xsamples = np.asarray(xsamples)\n",
    "ysamples = np.asarray(ysamples)\n",
    "\n",
    "#fitting each estimator\n",
    "for i, model in enumerate(models):\n",
    "    _X = xsamples[i, :]\n",
    "    _y = ysamples[i, :]\n",
    "    model.fit(_X, _y)\n",
    "    \n",
    "#make prediction and return the probabilities\n",
    "predictions = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "#we can use unpacking technique to remove the outerlist\n",
    "#we need [0] because .mode return both list and the counts\n",
    "[y_pred] = stats.mode(predictions)[0]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.93      0.86        68\n",
      "           1       0.93      0.80      0.86        82\n",
      "\n",
      "    accuracy                           0.86       150\n",
      "   macro avg       0.86      0.87      0.86       150\n",
      "weighted avg       0.87      0.86      0.86       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_estimators = 20\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#initially, we set our weight to 1/n\n",
    "W = np.ones(shape=n_samples) / n_samples\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(shape=n_estimators)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight=W)\n",
    "    \n",
    "    #compute the errors r_j\n",
    "    y_pred = model.predict(X_train) \n",
    "    compare = y_pred != y_train  #this works since True * w = 1 * w\n",
    "#     compare = np.array([int(x) for x in (y_pred != y_train)])\n",
    "    \n",
    "    r_j = np.sum(W * compare) / sum(W)\n",
    "    \n",
    "    #compute the predictor weight a_j\n",
    "    #let eta to be 1 as adaboost default \n",
    "    #if predictor is doing well, a_j will be big\n",
    "    eta = 1\n",
    "    a_j = eta * np.log ((1 - r_j) / r_j)\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(a_j)) / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = model.predict(X_test)\n",
    "    Hx += a_js[i] * y_pred\n",
    "    \n",
    "y_pred = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def mse(y, f):\n",
    "    return y - f\n",
    "\n",
    "def logit(y, f):\n",
    "    return y - expit(f)\n",
    "\n",
    "def fit(X, y, models, loss=\"mse\"):\n",
    "    \n",
    "    if loss == \"mse\":\n",
    "        loss_func = mse\n",
    "    else:\n",
    "        loss_func = logit\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='constant', constant=0)\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = loss_func(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting\n",
    "\n",
    "def change_to_0_1(sample):\n",
    "    return int(sample > 0.5)\n",
    "\n",
    "def predict_class(X, models):\n",
    "    probas = expit(predict(X, models))\n",
    "    return np.array([change_to_0_1(proba) for proba in probas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom MSE:  12.94555760240659\n",
      "Sklearn MSE:  12.945557601580582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Custom MSE: \", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom accuracy:  0.9532163742690059\n",
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models, loss=\"logistic\")\n",
    "\n",
    "#predict\n",
    "y_pred = predict_class(X_test, models)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Custom accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, y_pred_sk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9. Ensemble\n",
    "\n",
    "Ensemble is the idea of finding aggregated answers from multiple classifiers.  Indeed, Random Forests are one example of ensembles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n",
    "\n",
    "#### Hard voting\n",
    "Suppose you have trained a few classifiers, each one achieving about 80% accuracy.  A simple way to create a better classifier is to predict the class that gets the most votes.  This majority-vote classifier is called **hard voting** classifier.\n",
    "\n",
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy then ensemble.  In fact, even each classifier is a *weak learner* (i.e., could simply be a random guesser), their ensembles can be *strong learner*.\n",
    "\n",
    "One strong tip is that ensemble methods works best when **classifiers are independent**  Hence, the best practice is to use diverse classifiers which will make different types of errors, improving the overall accuracy of the ensemble.\n",
    "\n",
    "The voting classifier can be simply implemented using sklearn **VotingClassifier** API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "#hard voting\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting\n",
    "\n",
    "If all classifiers have a <code>predict_proba</code> method, then you can tell sklearn to predict the class with highest class probability, averaged over all individual classifiers.  This is called **soft voting**.  It gives usually better results because more weight is given to highly confident votes.  \n",
    "\n",
    "All we need to do is to replace <code>voting=hard</code> with <code>voting=soft</code>.  Also, since SVM does not have <code>predict_proba</code>, we need to set SVC of <code>probability=True</code> which will run cross-validation to get the probabilities and will give SVM the needed <code>predict_proba</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "#soft voting\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms as just discussed.  Another way is to use same training algorithms but with different random subsets of the training set (similar to Random Forests).  When sampling is performed **with** replacement, this is called **bagging** or **boostrapping**.  Otherwise, is called **pasting**.  In other words, only bagging allows training instances to be sampled several times for the same predictor.  \n",
    "\n",
    "And because bagging and pasting support parallel computing (e.g., using <code>n_jobs</code>), they are very popular methods.\n",
    "\n",
    "To perform in sklearn, we can use the <code>BaggingClassifier</code> API.  Pasting can be done using <code>BaggingClassifier</code> setting <code>boostrap=False</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging:  0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Bagging: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:  0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"Decision Tree: \", accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all.  By default <code>BaggingClassifier</code> samples roughly 70% of data, while leaving 30% untouched.  This untouched data is called **Out of Bag** (oob).  Note that oob is not the same for all predictors.\n",
    "\n",
    "One interesting is that since oob is something that each classifier never see, thus oob is somewhat a test set.  In <code>BaggingClassifier</code>, we can set <code>oob_score=True</code> which will evaluate the ensemble by averaging ut the oob evaluations of each predictor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, oob_score=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare with our prediction\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling features\n",
    "\n",
    "BaggingClassifier supports sampling the features as well.  This is controlled by two hyperparameters: <code>max_features</code> and <code>bootstrap_features</code>.  Thus each predictor will be trained on random subset of input features (becareful, I am talking about features, NOT instances)\n",
    "\n",
    "Sampling both training instances and features is called the **Random Patches**.  Keeping all training instances (bootstrap=False and max_samples=1.0) but sampling features (bootstrap_features=True and/or max_features = something less than 1.0) is called **Random Subspaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "**Boosting** refers to ensemble method that can combine several weak learners into a strong learners.  The general idea of most boosting methods is to train predictors **sequentially**, each trying to correct its predecessor.  The most popular ones are **AdaBoost** (short for Adaptive Boosting) and **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "\n",
    "By correcting what has been underclassified (or underfitted), we can create new predictor that focus on more hard cases.  Last, when we combine all sequential predictors, we get a predictor that takes care of all cases.\n",
    "\n",
    "For example, in the picture below, in the Weak Learner #1, a classification was made.  However, one red dot was misclassified.  This red dot gets increased weight and pass to the second learner.  The second learner now can correctly classify the bigger red dots, and then this bigger red dots become smaller (i.e., decrease weights).  At the same time, anything that is again misclassified here has increased weights and these weights are passed on to the next predictor.   And son on.  \n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging, except that predictors have different weights ($a_j$) depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "Initially, all data points ($w_i$) have same weight:\n",
    "\n",
    "$$ w_i = 1/N $$\n",
    "\n",
    "where N is the total number of data points, and the weighted samples always sum to 1, thus value of w of each point will always lie between 0 and 1.  \n",
    "\n",
    "We also can create weight for predictors (j) using\n",
    "\n",
    "$$ a_j = \\eta\\ln\\frac{1 - r_j}{r_j} $$\n",
    "\n",
    "where $\\eta$ is simply learning rate (defaults to 1), and $r_j$ is simply total number of misclassifications divided by the training set size.  This $a_j$ is useful both for (1) using in final predictions and (2) updating weights of each data samples.  To update each data sample (i) for the next predictor, we do:\n",
    "\n",
    "$$ \n",
    "  w_i =\n",
    "  \\begin{cases}\n",
    "    w_i, & \\text{if } \\hat{y_j^i} = y_i \\\\\n",
    "    w_{i-1}*\\exp(\\alpha_j) & \\text{otherwise } \\\\\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "In other words, new sample weight is simply function of old sample weight multiplied with Euler's number, raised to alpha we computed in (2).\n",
    "\n",
    "Last, once all predictions are trained, AdaBoost make predictions using\n",
    "based on the majority of weighted votes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/ada.png)\n",
    "Source: https://www.sciencedirect.com/topics/engineering/adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn implements AdaBoost using SAMME which stands for Stagewise Additive Modeling using a Multiclass Exponential Loss Function.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 Decision stumps.  A Decision stump is basically a Decision Tree with max_depth=1.  This is the default base estimator of AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

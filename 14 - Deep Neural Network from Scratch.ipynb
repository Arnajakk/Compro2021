{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 14 Deep Neural Network from Scratch\n",
    "\n",
    "### Readings\n",
    "\n",
    "- [WEIDMAN] Ch3\n",
    "- [CHARU] Ch2-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap, last time, we have inputted our data into a linear function wwhich we got some decent result.  To improve, we inserted a non-linear function in between follow by a linear function, which obviously increase the result since it help model the non-linearity.  We can summarize that neural newtork has basically the following key things that make it work:\n",
    "\n",
    "1. **Activation function (we can also generalized as Operations)**: these functions help model input data into a non-linear relationship\n",
    "\n",
    "2. **Chain rule / Backpropagation**: they are essential for us to improve the neural network\n",
    "\n",
    "3. **Layers of neurons**: they are performing some sequential processes that split out desired output.\n",
    "\n",
    "Putting together, the typial procedure of training a neural network is as followss:\n",
    "\n",
    "1. Feed observations/samples/records (X) into the model.  This step we called \"**forward pass**\"\n",
    "\n",
    "2. Calculate the loss \n",
    "\n",
    "3. Calculate gradients based on how each parameters (e.g., W, B) affect the loss by using chain rule.  This step was called \"**backward pass**\"\n",
    "\n",
    "4. Update the parameters (e.g., W, B) so that the loss will be hopefully be reduced in the next iteration.   This step was called \"**training**\"\n",
    "\n",
    "5. Stop when the loss does not decrease further by some tolerance level (e.g., 0.00001) or when it exceeds the specified maximum iteration.  Sometimes we called this \"**early stopping**\"\n",
    "\n",
    "In fact, you are now very close to understanding Deep Neural Networks.  In this lesson, we have several objectives:\n",
    "\n",
    "- From our low-level understandings of neural network, we shall code them up as a Python class, so they are resuable.  They will be essential for understanding deep neural network, CNN, and RNN.  You will be so surprised that all these fancy terms are simply layers after layers\n",
    "\n",
    "- When we code our work, we want to make sure these classes resemble PyTorch as much as possible, so you will understand PyTorch right away.\n",
    "\n",
    "- Of course, we shall also understand what is \"deep\" neural network.  Here, we shall simply say that \"deep\" neural network is simply neural network that has more than \"one\" hidden layers (which we did not yet define what is \"hidden\" layers)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Operations\n",
    "\n",
    "Let's first code up the first building block, the class <code>Operation</code>,  which is the operations/functions.  \n",
    "\n",
    "Each function has a **forward** and **backward** methods.  Forward methods for running the function and backward for calculating its gradients.\n",
    "\n",
    "Each of these functions receives an <code>ndarray</code> as input and outputs an <code>ndarray</code>.  In some operations such as matrix multiplication, we receive <code>ndarray</code> as <code>params</code>, thus we probably should have another class inheriting from <code>Operation</code> and allow for params as another instance variable.\n",
    "\n",
    "We also need to note that the shape of the output may vary.  For example, in matrix multiplication, the shape of output will be different from shape of input.  In sigmoid, input and output shares the same shape.  To make sure the shape is consistent, we can follow these facts:\n",
    "\n",
    "1. Each Operation will send outputs forward on the forward pass and will receive an “output gradient” on the backward pass, which will represent the partial derivative of the loss with respect to every element of the Operation’s output.  Thus **The shape of the output gradient ndarray must match the shape of the output.**\n",
    "\n",
    "2. On the backward pass, each Operation will send an “input gradient” backward, representing the partial derivative of the loss with respect to each element of the input.  **The shape of the input gradient that the Operation sends backward during the backward pass must match the shape of the Operation’s input.**\n",
    "\n",
    "![](figures/3-1.png)\n",
    "\n",
    "![](figures/3-2.png)\n",
    "\n",
    "Based on this, we can write the class Operation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "class Operation(object):\n",
    "  \n",
    "    #nothing to init\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    #forward receive ndarray as input\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        #put trailing _ to avoid naming conflict\n",
    "        self.input_ = input_\n",
    "\n",
    "        #this _output will use self.input_ to calculate the ouput\n",
    "        #_  here means internal use\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        #perform input grad based on output_grad\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        #input grad must have same shape as input\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add also another class that inherits from <code>Operation</code> that we’ll use specifically for Operations that involve parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__()  #inherit from parent if any\n",
    "        self.param = param  #this will be used in _output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        #make sure output and output_grad has same shape\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        #perform gradients for both input and param\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement some functions that we implement in last class, including:\n",
    "1. Matrix multiplication\n",
    "2. Addition of bias term\n",
    "3. Sigmoid activation function\n",
    "\n",
    "Lets start with matrix multiplication.  Since the input has two params, X and W, we inherit from <code>ParamOperation</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        #initialize Operation with self.param = W\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ @ self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad @ self.param.T  #same as last class\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        return self.input_.T @ output_grad  #same as last class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the BiasAdd operation where the gradients are simply one.  Since it is an operation between X and B, we inherit from <code>ParamOperation</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self, B: ndarray):\n",
    "        #initialize Operation with self.param = B.\n",
    "        assert B.shape[0] == 1  #make sure it's only B\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do sigmoid.  Since sigmoid is simply a operation that maps to another value, it inherits from Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return 1.0/(1.0 + np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Layers\n",
    "\n",
    "In terms of <code>Operations</code>, <code>layers</code> are a series of linear operations followed by a nonlinear operation. For example, our neural network from the last chapter could be said to have had five total operations: two linear operations — a weight multiplication and the addition of a bias term — followed the sigmoid function and then two more linear operations.\n",
    "\n",
    "![](figures/layer.png)\n",
    "\n",
    "Here, we define the input as **input layer**, Layer 1 is typically called **hidden layer** because it is the only layer whose values we don't typically see explicitly during the course of training.  Layer 2 is typically called **output layer** which outputs the desired value.\n",
    "\n",
    "By abstraction, we can make neural network look much simpler as follows:\n",
    "\n",
    "![](figures/layer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer can be said to have a certain number of neurons equal to the dimensionality of the vector that represents each observation in the layer’s output. The neural network from the last class can thus be thought of as having 13 neurons in the input layer (i.e., 13 features), then 13 neurons (again) in the hidden layer, and one neuron in the output layer.\n",
    "\n",
    "Neurons in the brain have the property that they can receive inputs from many other neurons and will “fire” and send a signal forward only if the signals they receive cumulatively reach a certain “activation energy.” Neurons in the context of neural networks have a loosely analogous property: they do indeed send signals forward based on their inputs, but the inputs are transformed into outputs simply via a nonlinear function. Thus, this nonlinear function is called the activation function, and the values that come out of it are called the activations for that layer.\n",
    "\n",
    "**Building on the context of layers, deep learning models are simply neural networks with more than one hidden layer.**\n",
    "\n",
    "Now leaving all the theory behind, let's code the Layer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.first = True   #first layer is true for init\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int):\n",
    "        #setup the series of operations\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        #setup self.operations if haven't\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        #run the series of operations\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    #if the operation is a subclass of ParamOperatio\n",
    "    #append param_grad to self.param_grads\n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our layer.  Remember that we have three layers:\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer\n",
    "3. Output layer\n",
    "\n",
    "We don't really need to implement the input layer since it's only the input.  \n",
    "\n",
    "As for our hidden layer, it composes of WeightMultiply, then BiasAdd, then sigmoid.   What name should we give to this layer?  How about LinearNonLinear layer.  In fact, there is a common name for this is \"**Dense/Fully-Connected Layer**\" which refers to layer where each output neuron is a function of all of the input neurons.   Imagine thirteen circles, each circle connected to all circles...(that's why it's called fully-connected)\n",
    "\n",
    "Our output layer is very similar to the hidden layer but without the hidden layer.  We consider this still as a **Dense** layer because each output neuron is again connected to all input neurons.\n",
    "\n",
    "To code this is simple, we simply inherit **Layers** and define the series of operations in <code>_setup_layer</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        #define the desired non-linear function as activation\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        #in case you want reproducible results\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # randomize weights of shape (num_feature, num_neurons)\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # randomize bias of shape (1, num_neurons)\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss Class\n",
    "\n",
    "The next thing we have to code up is the loss function (forward) and its gradients (backward).  We gonna make a parent class called <code>Loss</code> and a child class called <code>MeanSquaredError</code>  The code is quite straightforward, similar to Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        #self._output will hold the loss function\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        #input_grad will hold the gradient of the loss function\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Loss/Objective/Cost function, let's make the concrete loss function.  Here we will be using the <code>MeanSquaredError</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

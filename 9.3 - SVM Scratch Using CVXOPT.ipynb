{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Credits: http://cvxopt.org\n",
    "\n",
    "Let's work on both the hard margin and soft margin SVM algorithm in Python using the well known **CVXOPT** library. While the algorithm in its mathematical form is rather straightfoward, its implementation in matrix form using the CVXOPT API can be challenging at first. This notebook will show the steps required to derive the appropriate vectorized notation as well as the inputs needed for the API.\n",
    "\n",
    "#### Notations\n",
    "\n",
    "Scalars are denoted with italic lowercases (e.g., $y$, $b$), vectors with bold lowercases (e.g., $\\mathbf{w},\\, \\mathbf{x}$), and matrices with italic uppercases (e.g., $W$). $\\mathbf{w^T}$ is the transpose of $\\mathbf{w}$\n",
    "and $\\|\\mathbf{w}\\| = \\mathbf{w}^T\\mathbf{w}$\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\mathbf{x}$ be a feature vector (i.e., the input of the SVM). $\\mathbf{x} \\in \\mathbb{R}^n$, where $ùëõ$ is the dimension of the feature vector. \n",
    "- $y$ be the class (i.e., the output of the SVM). $y \\in \\{ -1,1\\}$, i.e. the classification task is binary.\n",
    "- $\\mathbf{w}$ and $b$ be the parameters of the SVM: we need to learn them using the training set.\n",
    "- $(\\mathbf{x}^{(i)}, y^{(i)})$ be the $ùëñ$th sample in the dataset. Let's assume we have $m$ samples in the training set.\n",
    "\n",
    "With n = 2, one can represent the SVM's decision boundaries as follows:\n",
    "\n",
    "<img src = \"figures/svm.png\" width=\"300\" >\n",
    "\n",
    "The class $y$ is determined as follows:\n",
    "\n",
    "$$\n",
    "y^{(i)}=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\leq -1 \\\\\n",
    "                  1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\ge 1 \\\\\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$$\n",
    "\n",
    "which can be more concisely written as \n",
    "\n",
    "$$y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1$$\n",
    "\n",
    "#### Goal\n",
    "\n",
    "The SVM aims at satisfying two requirements:\n",
    "\n",
    "1. The SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to maximize the distance between the hyperplane defined by $\\mathbf{w^T}\\mathbf{x}+b = -1$ and the hyperplane defined by $\\mathbf{w^T}\\mathbf{x}+b = 1$.  This distance is equal to $\\frac{2}{\\|\\mathbf{w}\\|}$.  This means we want to solve $\\underset{\\mathbf{w}}{\\operatorname{max}} \\frac{2}{\\|\\mathbf{w}\\|}$.  Equivalently we want $\\underset{\\mathbf{w}}{\\operatorname{min}} \\frac{\\|\\mathbf{w}\\|}{2}$\n",
    "\n",
    "2. The SVM should also correctly classify all $\\mathbf{x}^{(i)}$, which means \n",
    "\n",
    "$$y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1, \\forall i \\in \\{1,\\dots,m\\}$$\n",
    "\n",
    "#### Hard margin vs. Soft margin \n",
    "\n",
    "This is the **hard-margin SVM**, as this quadratic optimization problem admits a solution iff the data is linearly separable.\n",
    "\n",
    "One can relax the constraints by introducing so-called **slack variables** $\\xi^{(i)}$. Note that each sample of the training set has its own slack variable. This gives us the **soft-margin SVM** following quadratic optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}+ C \\sum_{i=1}^{N} \\xi^{(i)}, \\\\\n",
    "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1 - \\xi^{(i)},&\\forall i \\in \\{1,\\dots,N\\} \\\\\n",
    "\\quad&\\xi^{(i)}\\ge0, &\\forall i \\in \\{1,\\dots,N\\} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "One can add even more flexibility by introducing a function $\\phi$ that maps the original feature space to a higher dimensional feature space. This allows non-linear decision boundaries. The quadratic optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}+ C \\sum_{i=1}^{N} \\xi^{(i)}, \\\\\n",
    "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\phi \\left(\\mathbf{x}^{(i)}\\right)+b) \\ge 1 - \\xi^{(i)},&\\forall i \\in \\{1,\\dots,N\\} \\\\\n",
    "\\quad&\\xi^{(i)}\\ge0, &\\forall i \\in \\{1,\\dots,N\\} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Primal Problem\n",
    "\n",
    "Recall that our primal, optimization problem is of the form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\min_{w, b} f(w,b) & = \\min_{w, b}  \\  \\frac{1}{2} ||w||^2\n",
    "\t\\\\\n",
    "\ts.t. \\ \\  g_i(w,b) &= - y^{(i)} (w^T x^{(i)} + b) + 1 = 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Lagrange Method\n",
    "\n",
    "The method of Lagrange multipliers allows us to turn a constrained optimization problem into an unconstrained one of the form:\n",
    "\n",
    "$$\\mathcal{L}(w, b, \\alpha) =   \\frac{1}{2} ||w||^2 - \\sum_i^m \\alpha_i [y^{(i)} (w^T x^{(i)} + b) - 1]$$\n",
    "\n",
    "Where $\\mathcal{L}(w, b, \\alpha)$ is called the Lagrangian and $\\alpha_i$ are called the Lagrangian multipliers.\n",
    "\n",
    "Our primal optimization problem with the Lagrangian becomes the following:\n",
    "\n",
    "$$\\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "#### Dual Problem\n",
    "\n",
    "This is the idea of turning primal problem into dual problem by acknowledging that this is roughly the same:\n",
    "\n",
    "$$\\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right) =   \\max_\\alpha \\left( \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "This allows us to take the partial derivatives of $\\mathcal{L}(w, b, \\alpha)$ with respect to $w$ and $b$\n",
    ", equate to zero and then plug the results back into the original equation of the Lagrangian\n",
    "\n",
    "$$\\mathcal{L}(w, b, \\alpha) =   \\frac{1}{2} ||w||^2 - \\sum_i^m \\alpha_i [y^{(i)} (w^T x^{(i)} + b) - 1]$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial {\\mathbf w} }= {\\mathbf w} - \\sum \\alpha_i \\; y_i \\; \\mathbf x_i = 0$$\n",
    "\n",
    "$${\\mathbf w} = \\sum \\alpha_i \\; y_i \\; \\mathbf x_i\\tag 6$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b}=-\\sum \\alpha_i y_i = 0,$$\n",
    "\n",
    "$$\\sum \\alpha_i \\, y_i = 0\\tag 7$$\n",
    "\n",
    "Plugging the w back to the original equation, as well as the fact that $-\\sum \\lambda_i y_i = 0,$, we got that \n",
    "\n",
    "$$\\mathcal{L}(w, b, \\alpha) = \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}> $$\n",
    "\n",
    "hence generating an equivalent dual optimization problem of the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\max_{\\alpha} \\min_{w,b} \\mathcal{L}(w,b,\\alpha)\n",
    "\t\\\\\n",
    "\t& \\max_{\\alpha} \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}> \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\alpha_i \\geq 0 \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\sum_i^m \\alpha_i y^{(i)} = 0\t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Why Dual?\n",
    "\n",
    "Short answer: Faster computation + allows to use the kernel trick.  Though, there exist some good methods to train SVM in the primal form (See my Assignments 6 - SVM in primal form)\n",
    "\n",
    "#### Duality and KTT\n",
    "\n",
    "Karush Kuhn Tucker (KTT) conditions allow us to solve the dual problem instead of the primal one, while ensuring that the optimal solution is the same. In our case the conditions are the following:\n",
    "\n",
    "- The primal objective and inequality constraint functions must be convex\n",
    "- The equality constraint function must be affine\n",
    "- The constraints must be strictly feasible\n",
    "\n",
    "Then there exists $w^*$, $\\alpha^*$ which are solutions to the primal and dual problems. Moreover, the parameters $w^*$, $\\alpha^*$ satisfy the KTT conditions below:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\frac{\\partial}{\\partial w_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(A)\n",
    "\t\\\\\n",
    "\t&\\frac{\\partial}{\\partial \\beta_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(B)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* g_i(w^*) = 0 &(C)\n",
    "\t\\\\\n",
    "\t&g_i(w^*) \\leq 0  &(D)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* \\geq 0 &(E)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Moreover, if some $w^*$, $\\alpha^*$ satisfy the KTT solutions then they are also solution to the primal and dual problem.\n",
    "\n",
    "Equation $(C)$ above is of particular importance and is called the *dual complementarity* condition. It implies that if $\\alpha_i^* > 0$ then $g_i(w^*) = 0$ which means that the constraint $g_i(w^*) \\leq 0$ is active, i.e., it holds with equality rather than inequality.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\min \\frac{1}{2} x^TPx + q^Tx\n",
    "    \\\\\n",
    "     s.t. \\ & \\ Gx \\leq h \n",
    "    \\\\\n",
    "    & \\ Ax = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}>\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\max_{\\alpha} \\sum_i^m \\alpha_i  - \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha\n",
    "    \\\\\n",
    "     s.t. & \\ \\alpha_i \\geq 0 \n",
    "    \\\\\n",
    "    &  \\ \\sum_i^m \\alpha_i y^{(i)} = 0  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\min_{\\alpha}  \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha - 1^T \\alpha\n",
    "    \\\\\n",
    "    & s.t. \\ - \\alpha_i \\leq 0 \n",
    "    \\\\\n",
    "    & s.t. \\ y^T \\alpha = 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} x_1^{(1)} & x_2^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} \\end{bmatrix} \\ \\ \\ y = \\begin{bmatrix} y^{(1)}  \\\\ y^{(2)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' = \\begin{bmatrix} x^{(1)}_1 y^{(1)} & x^{(1)}_2y^{(1)} \\\\\n",
    "x^{(2)}_1y^{(2)} & x^{(2)}_2y^{(2)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Making a prediction\n",
    "Once the $\\alpha^{(i)}$ are learned, one can predict the class of a new sample with the feature vector <code>X_test</code> as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y^{\\text {test}}&=\\text {sign}\\left(\\mathbf{w^T}\\phi\\left(\\mathbf{x}^{\\text {test}}\\right)+b\\right) \\\\\n",
    "&= \\text {sign}\\left(\\sum_{i =1}^{m}\\alpha^{(i)}y^{(i)}\\phi\\left(x^{(i)}\\right)^T\\phi\\left(\\mathbf{x}^{\\text {test}}\\right)+b \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
